#beats.conf
input {
    beats {
        port => 1234
        }
} 
output {
    elasticsearch {
    }
}
И
спользование флага -r во время работы Logstash позволяет вам
автоматически запускать конфигурацию, если она была изменена и
сохранена. Это может быть полезно во время тестирования новых
конфигураций без необходимости вручную перезапускать Logstash
каждый раз, когда конфигурация изменилась.

IMAP 
#email_log.conf
input {
imap {
host => "imap.packt.com"
password => "secertpassword"
user => "user1@pact.com"
port => 993
check_interval => 10
folder => "Inbox"
}
} 
output {
stdout {
codec => rubydebug
} 
elasticsearch {
index => "emails"
document_type => "email"
hosts => "localhost:9200"
}
}
По умолчанию плагин logstash-input-imap считывает
сообщения из папки INBOX и опрашивает сервер IMAP каждые 300
секунд. В приведенной выше конфигурации при указании
параметра check_interval задается пользовательский интервал
каждые 10 с.
Если индекс по умолчанию не указан, то шаблон индекса будет
logstash-%(+YYYY.MM.dd), а document_type будет задан как тип
события, даже если он уже существует, в ином случае тип документа
будет назначен значением логов/событий.

Рассмотрим простой пример с базовой конфигурацией
PagerDuty. Она настроена таким образом, чтобы Elasticsearch
запрашивала в индексе "ngnixlogs" все документы,
соответствующие statuscode:404, и для каждого найденного
документа поднимались события pagerduty:

#kafka.conf
input {
elasticsearch {
hosts => "localhost:9200"
index => "ngnixlogs"
query => '{ "query": { "match": {
"statuscode": 404} }}'
}
} 
output {
pagerduty {
service_key => "service_api_key"
details => {
"timestamp" => "%{[@timestamp]}"
"message" => "Problem found: %{[message]}"
}
 event_type => "trigger"
}
}


ожественные события (по одному на каждый элемент).
Пример простого применения плагина JSON выглядит
следующим образом:
input{
stdin{
codec => "json"
}
} Е
сли у вас несколько записей JSON, разделенных \n, необходимо
использовать кодек json_lines.
Если кодек JSON получает данные из ввода, состоящего из
некорректных документов JSON, они будут обработаны как простой
текст с добавлением тега _jsonparsefailure.

Если Elasticsearch запущена в конфигурации по умолчанию, она
будет работать как главный узел, узел данных и узел поглощения.
Для отключения функции поглощения измените следующую
настройку в файле elasticsearch.yml:
node.ingest: false

Для этого мы попросту указываем параметр pipeline
в индексе или в составном запросе, чтобы узел поглощения знал,
какой контейнер использовать:
POST my_index/my_type?pipeline=my_pipeline_id
{
"key": "value"
}


Рассмотрим пример. Как видно в следующем коде, мы
определили новый контейнер с названием firstpipeline,
который переводит значения поля message в верхний регистр:
curl -X PUT
http://localhost:9200/_ingest/pipeline/firstpipeline
-H
'content-type: application/json'
-d '{
"description" : "uppercase the incoming value in
the message field",
"processors" : [
{
"uppercase" : {
"field": "message"
}
}
]
}'



Фильтр даты
Этот плагин используется для обработки даты в полях. Он очень
удобен и полезен при работе с временными рядами событий. По
умолчанию Logstash добавляет поле @timestamp для каждого
события, оно содержит время обработки события. Но пользователю
может быть интересно не время обработки, а фактическое время,
когда произошло событие. Таким образом, с помощью данного
фильтра мы можем обрабатывать метку даты/времени из полей и
далее применять ее как метку времени самого события.
Возможно использовать плагин следующим образом:
filter {
date {
match => [ "timestamp", "dd/MMM/YYYY:HH:mm:ss
Z" ]
}
}
По умолчанию фильтр даты перезаписывает поле @timestamp,
но вы можете это изменить, явно указав целевое поле, как показано
во фрагменте кода ниже. Пользователь также может оставить время,
обработанное Logstash:
filter {
date {
match => [ "timestamp", "dd/MMM/YYYY:HH:mm:ss
Z" ]
target => "event_timestamp"
}
}